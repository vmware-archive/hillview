\documentclass{article}

\usepackage{qtree}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz-cd}
\usepackage{xspace}

\title{A Formal Model of Hillview}
\date{June 2020}
\author{}

\newcommand{\tree}[1]{\mathit{Tree}\langle #1 \rangle}
\newcommand{\mr}[1]{\ensuremath{\mathrm{\mathbf{#1}}}}
\newcommand{\N}{\ensuremath{\mathbb{N}}\xspace}
\newcommand{\B}{\ensuremath{\mathbb{B}}\xspace}
\newcommand{\E}{\ensuremath{\mathcal{E}}\xspace}

% Use symbols for footnotes so they are not confused with exponents
\makeatletter
\newcommand*{\myfnsymbolsingle}[1]{%
  \ensuremath{%
    \ifcase#1% 0
    \or % 1
      \dagger
    \or % 2
      \ddagger
    \or % 3
      \mathsection
    \or % 4
      \mathparagraph
    \else % >= 5
      \@ctrerr
    \fi
  }%
}
\makeatother

\newcommand*{\myfnsymbol}[1]{%
  \myfnsymbolsingle{\value{#1}}%
}

% remove upper boundary by multiplying the symbols if needed
\usepackage{alphalph}
\newalphalph{\myfnsymbolmult}[mult]{\myfnsymbolsingle}{}

\renewcommand*{\thefootnote}{%
  \myfnsymbolmult{\value{footnote}}%
}

\begin{document}
\maketitle

\section{Basics}

\begin{table}[h]
\begin{tabular}{ll}
  Notation & Meaning \\ \hline
  $M$ & Set of machines. \\
  \E & Environment. \\
  $D$ & Type of data stored in a tree. \\
  $\tree{D}$ & Tree with data of type $D$ in leaves. \\
  $t$ & A tree instance. \\
  $(a, b)$ & The pair with values $a$ and $b$. \\
  $l$ & A leaf node in a tree. \\
  $i$ & An internal node in a tree. \\
  $R$ & Monoid (set with associative operation $+_R$ and a zero $0_R$); results produced by sketches. \\
  $A[B]$ & Partial functions (key-value maps) from $B$ to $A$, where $A$ and $B$ are sets \\
  $\N[A]$ & A multiset over $A$. \\
  $T^*$ & Lists of values of type $T$. \\
  $[]$ & The empty list. \\
  $T$ & A ``tuple''; usually $D = T^*$. \\
  $|t|$ & The interpretation of a tree as a multiset $\N[D]$. \\
  $x \mapsto x + 1$ & A function that maps $x$ to $x + 1$. \\
  $\{ a \mapsto 2 \}$ & A multiset where the value $a$ appears twice. \\
  \hline
\end{tabular}
\caption{Notations used in this document.}
\end{table}

$M$ is a countable set of machines.  $S$ are strings over some alphabet.

$D$ denotes a type; the set of types includes (), \N, \B\ --- Booleans,
strings, product types, sum types, etc.  The type $A[B]$ is the type
of finite maps from $B$ to $A$.

We will use often the ``object oriented'' suffix notation for
functions: $a.f(x)$ is the same as $f(a, x)$.  A function with only 1
argument is also called a ``property'' and can be shown without
parentheses in the suffix notation: $a.f$.

\E is the ``environment'' that stores data on each machine.  The
environment provides a set of functions to read the data (think of
reading the data from a file): $\E.\mr{read}_D : M \rightarrow S
\rightarrow D$, where $d: D = \E.\mr{read}(m, \mr{'people'})$ is the
data (of type $D$) residing on machine $m$ in the file named
$\mr{'people'}$.

We define two handy functions: $\mr{one}_R: R \rightarrow \N$ (for an
arbitrary $R$), defined by $\mr{one}(x) = 1$, and the identity
function $\mr{id}_R: R \rightarrow R$, $\mr{id}(x) = x$.

\section{Computation trees}

We deal with typed \emph{computation trees}: $\tree{D}$ is the set of
trees with a value of type $D$ in each leaf node (all leaves of a tree
must have the same type of data).

Nodes $N_D$ in a tree of type $D$ are either internal nodes $I_D$ or
leaves $L_D$, so $N_D = I_D \oplus L_D$.  A tree is defined by its
root $t \in N_D$.  Every tree node has a \mr{machine} property, which
is the machine where the tree node resides: $\mr{machine}: N_D
\rightarrow M$.

In addition, each leaf node has a \mr{data} property: the data value
it stores locally $\mr{data}: L_D \rightarrow D$.

The children of an internal node $n \in I_D$ are given by a finite
partial function $\mr{children} : I_D \rightarrow N_D[M]$ that takes a
machine and returns a child node that must reside on that machine (for
any $i \in I_D, i.\mr{children}(m).\mr{machine} = m$).  An internal
node can have zero children.  In the following figure we display a
tree $t \in \tree{\N}$; we show for each node its machine $m_x$; for
leaf nodes we also display the data value, a number:

\Tree [ .$m_0$ [ .$m_1$ [ .$m_2(0)$ ] [ .$m_3(1)$ ] ] [ .$m_4(2)$ ]]

If we do not particularly care about the machine where a node resides;
then we can display just the leaf values:

\Tree [ . [ . [ .0 ] [ .1 ] ] [ .2 ]]

We will consider an \emph{interpretation function} $|.|$ over trees
that maps a tree to a multiset of values over $D$, the multiset of the
values in all leaves.  $|.| : \tree{D} \rightarrow \N[D]$, defined
recursively as follows:

For a leaf node $|l| = \{ l.\mr{data} \mapsto 1 \}$ (the \mr{data}
value with a count of 1).  For an internal node: $|i| = \cup_{c \in
  i.\mr{children}} |c|$, where we use the multiset union.

\subsection{Transforming trees (map)}

Given a function $f: D_0 \rightarrow D_1$, the \mr{map} operator
produces a function between trees: $\mr{map}: (D_0 \rightarrow D_1)
\rightarrow \tree{D_0} \rightarrow \tree{D_1}$, defined as follows:

For a leaf node: $l \in L_{D_0}$, $l.\mr{map}(f).\mr{data} =
f(l.\mr{data})$, and $l.\mr{map}(f).\mr{machine}$ = $l.\mr{machine}$
(the result is a leaf on the same machine, with data produced by
applying $f$ to the existing data).

For an internal node $i \in I_{D_0}$ we have recursively
$i.\mr{map}(f).\mr{children}(m) = i.\mr{children}(m).\mr{map}(f)$ and
$i.\mr{map}(f).\mr{machine} = i.\mr{machine}$.  (same machine,
children produced recursively).

If $t$ is the tree defined above, $t.\mr{map}(x \mapsto x + 1)$
produces $t_1$:

\Tree [ .$m_0$ [ .$m_1$ [ .$m_2(1)$ ] [ .$m_3(2)$ ] ] [ .$m_4(3)$ ]]

Note that each node resides on the same machine as the ``source'' node
that it was produced from.

\subsection{Growing trees (flatmap)}

Let us consider a function that given a leaf node produces a tree :
$f: L_{D_0} \rightarrow \tree{D_1}$.  Note that this function can
``use'' the environment of the node to produce it's result (e.g., by
reading the list from the environment); all the leaves in the result
must have the same type $D_1$.

We can define the \mr{flatmap} operator, which produces a function
between trees: $\mr{flatmap}: (L_{D_0} \rightarrow \tree{D_1}) \rightarrow
\tree{D_0} \rightarrow \tree{D_1}$, defined as follows:

For a leaf node $l \in L_{D_0}$ \mr{flatmap} produces a new internal
node \\ $l.\mr{flatmap}(f).\mr{children}(m) = f(l)$, where in the tree
produced by $f$ each new node resides on the ``right'' machine.

For an internal node $i \in I_{D_0}$ we have recursively
$i.\mr{flatmap}(f).\mr{children}(m) = i.\mr{children}(m).\mr{flatmap}(f)$.

As a example, consider a flatmap operator that takes as an argument
function that transforms the leaf node on machine $m_4$ into the
sub-tree $[m_5 \mapsto 3, m_6 \mapsto 1]$ and does not change the other
leaves.  Applied to $t$ this produces:

\Tree [ .$m_0$ [ .$m_1$ [ .$m_2(0)$ ] [ .$m_3(1)$ ] ] [ .$m_4$ [ .$m_5(3)$ ] [.$m_6(1)$ ]]]

\subsection{Combining multiple trees (zip)}

Given two isomorphic trees $t_0 \in \tree{D_0}$ and $t_1 \in
\tree{D_1}$ with the corresponding nodes on the same machines, and a
function $c : D_0 \times D_1 \rightarrow D$, we can define the zip of
the two trees as follows:

For leaf nodes $\mr{zip}(c)(l_0, l_1).\mr{data} = c(l_0.\mr{data}, l_1.\mr{data})$.

For internal nodes the zip is applied recursively to corresponding
children: $\mr{zip}(c)(i_0, i_1) = [ m \mapsto \mr{zip}(c)(d_0, d_1) |
  m \in M \land ((m \mapsto d_0) \in i_0) \land ((m \mapsto d_1) \in
  i_1) ]$.

As an example, let us apply $\mr{zip}( (x,y) \mapsto x+y )$ to the trees
$t$ and $t_1$ to obtain:

\Tree [ .$m_0$ [ .$m_1$ [ .$m_2(1)$ ] [ .$m_3(3)$ ] ] [ .$m_4(5)$ ]]

(I believe that zip can be generalized to work with non-isomorphic
trees by treating missing nodes as a particular tree that has no
leaves, and missing leaves as having a distinguished value
representing an empty set with values in $D$.)

\subsection{Shrinking trees (prune)}

Given a function $e : D \rightarrow \B$, we define a \mr{prune}
operator on trees that removes leaves where $e$ returns ``false'':

For a leaf node \[l.\mr{prune}(e) =
\begin{cases}
  l & \mbox{ if } e(l.\mr{data}) \\
  \phi & \mbox { otherwise}
\end{cases}
\].

For an internal node: $i.\mr{prune}(e).\mr{children} = \cup_{c \in i.\mr{children}} c.\mr{prune}(e)$.

\section{Aggregating tree data}

So far all the operations that we had were converting trees into
trees.  Aggregation is the only operation that can produce a
``simple'' value from a tree: a summary of the data stored in the
tree's leaves.

\subsection{Sketches}

Consider a set of values $D$ and a monoid $(R, +, 0)$ (most often we
work with commutative monoids, but commutativity is not required).  A
\emph{sketch} is an operator $\mr{sketch}(s_R)$ defined by a function
$s_R : D \rightarrow R$.  Note that the monoid $R$ is part of the
sketch definition; when this is clear from the context we write $s$,
omitting the subscript $R$.  But one can define multiple monoids over
the same underlying set (e.g., for $\N$ the operation can be either
addition or ``max''), so it is very important to understand which
monoid is being used to compute a specific sketch.  Given a tree $t
\in \tree{D}$, we define a \emph{sketch operator} that transforms
trees into values in $R$ as a function $\mr{sketch}(s) : \tree{D}
\rightarrow R$ defined recursively as follows:

For leaves $l.\mr{sketch}(s) = s(l.\mr{data})$.

For internal nodes $i.\mr{sketch}(s) = \sum_{c \in i.\mr{children}}
c.\mr{sketch}(s)$, where the sum is done using the monoid operation.
Notice that the machine where the nodes reside has no effect on the
sketch result.

As an example, consider the monoid $(\N, +, 0)$ of natural numbers
with addition.  Then we can apply the sketch operator to the above
tree $t$ with the identity function as an argument to obtain the sum
of values in all leaves $t.\mr{sketch}(\mr{id}) = 0 + 1 + 2 = 3$.

If we consider sketches as being applied to tree interpretations we
notice that all sketches are linear transformations between $\N[D]$
and $R$.  If the monoid $R$ is commutative then the result of applying
a sketch to a tree does not depend on the ordering of children.

\section{Properties of operators}

The following commutative diagrams describe properties of these operators:

\begin{tikzcd}
  t_0 \arrow{r}{\mr{map}(f)} \arrow[swap]{dr}{\mr{map}(g \circ f)}
  & t_1 \arrow{d}{\mr{map}(g)} \\
  & t_3
\end{tikzcd}

\begin{tikzcd}
  t_0 \arrow{r}{\mr{map}(f)} \arrow[swap]{dr}{\mr{flatmap}(g \circ f)}
  & t_1 \arrow{d}{\mr{flatmap}(g)} \\
  & t_3
\end{tikzcd}

\begin{tikzcd}
  t_0 \arrow{r}{\mr{map}(f)} \arrow[swap]{dr}{\mr{sketch}(g \circ f)}
  & t_1 \arrow{d}{\mr{sketch}(g)} \\
  & r
\end{tikzcd}

\begin{tikzcd}
  & t_1 \arrow[dashrightarrow]{drr}{} \\
  t_0 \arrow{ru}{\mr{map}(f)} \arrow[swap]{rd}{\mr{map}(g)} \arrow{rr}{\mr{map}(x \mapsto p(f(x), g(x)))}
   & & t_3
   & (t_1, t_2) \arrow[Rightarrow,swap]{l}{\mr{zip}(p)} \\
  & t_2 \arrow[dashrightarrow]{rru}{}
\end{tikzcd}

\begin{tikzcd}
  & r_1 \arrow[dashrightarrow]{drr}{} \\
  t_0 \arrow{ru}{\mr{sketch}(f)} \arrow[swap]{rd}{\mr{sketch}(g)} \arrow{rrr}{\mr{sketch}(x \mapsto (f(x), g(x)))}
   & & & (r_1, r_2) \\
  & r_2 \arrow[dashrightarrow]{rru}{}
\end{tikzcd}

\section{Additional structure on $D$}

We will now put additional structure on $D$; in particular, we will
consider types where $D$ is a list of values over a base type $T$,
i.e., $D = T^*$.  In this case we can also extend the interpretation
function of a tree to treat it as a multiset over $T$: $|.|:
\tree{T^*} \rightarrow \N[T]$.  With this interpretation sketches are
still linear transformations between tree interpretations and the
result monoid.

Given this structure we can define a few more interesting operations
on trees in terms of the existing operations.

As an example, consider a tree $t_2 \in \tree{\N^*}$, where each
leaf contains a list of integers:

\Tree [ . [ . [ .\{1,1\} ] [ .\{1,2\} ]] [ .\{3,4\} ]]

\subsection{Maps}

Recall that the \mr{map} operators takes a function $m : D_0
\rightarrow D_1$; now let $D_0 = T_0^*$ and $D_1 = T_1^*$.  Given a
function $f : T_0 \rightarrow T_1$ we can perform the map of a map:
$t_2.\mr{map}(\mr{map}(x \mapsto x + 1))$, where the inner map is the
classic map operator defined over lists.  This produces the
following result:

\Tree [ . [ . [ .\{2,2\} ] [ .\{2,3\} ]] [ .\{4,5\} ]]

\subsection{Filters}

Consider a predicate $p : T \rightarrow \B$.  Define $\mr{filter}(p) :
T \rightarrow T^*$ as \[
\mr{filter}(p)(e) =
\begin{cases}
  \{e\} & \mbox{ if } p(e) \\
  \phi & \mbox{ (empty set) otherwise }
\end{cases}
\].

We have the classic list filtering operator: $\mr{filter} : (T
\rightarrow \B) \rightarrow T^* \rightarrow T^*$:
$\mr{filter}(p)(v) =
\begin{cases}
  [] & \mbox{ if } v = [] \\
  \mr{cons}(\mr{head}(v), \mr{filter}(p)(\mr{tail}(v))) & \mbox{ if } p(\mr{head}(v)) \\
  \mr{filter}(p)(\mr{tail}(v)) & \mbox{ otherwise}
\end{cases}
$.

We can also use the tree \mr{map} operators to perform filtering on
tree leaves: $\mr{filter}(p)(t) = t.\mr{map}(\mr{filter}(p))$.

For example, given the predicate $p: \N \rightarrow \B = n \mapsto n
\pmod 2 = 0$, the expression $t_2.\mr{map}(\mr{filter}(p))$ produces
the following result:

\Tree [ . [ . [ .\{\} ] [ .\{2\} ]] [ .\{4\} ]]

\subsection{Aggregates}

We can compute interesting aggregate functions over the leaves of a
tree using the tree \mr{sketch} operator.  For example, to compute the
count of elements in a tree we can use the following ``count'' sketch:
$R = (\N, +, 0)$, $\mr{count} : T^* \rightarrow \N$, $\mr{count}(x) =
\sum_{e \in x} 1 = \sum_{e \in x} \mr{one}(e)$.  We can count the
number of values in $t_2$ as follows: $t_2.\mr{sketch}(\mr{count})$ to
get the result 6 (there are 6 values in the leaves).

To compute the average value of the leaves we can use a more complex
sketch function that computes a tuple composed of the sum and the
count: $\mr{avg} = x \mapsto (\sum_{e \in x} \mr{id}(e),
\mr{count}(x))$; by dividing the sum by the count we can obtain the
average.  The sketch $\mr{sketch}(\mr{avg}): \tree{\N} \rightarrow \N
\times \N$, where addition is applied pointwise.

\subsection{Incremental sketches}

Consider a function $\mr{inc}: T \rightarrow R$, where $R$ is a
monoid.  We can define recursively the operator $\mr{fold}(\mr{inc}):
T^* \rightarrow R$ as \\ $\mr{fold}(v) =
\begin{cases}
  0_R & \mbox{ if } v = [] \\
  f(head(v)) +_R \mr{fold}(\mr{inc})(tail(v)) & \mbox{ otherwise}
\end{cases}
$.

We call such functions ``increments'', because they define the
contribution of an element $t \in T$ to the result $R$.  Each
increment function can be used to define an \emph{incremental sketch}
operator: $\mr{incsketch}_R(\mr{inc}) =
\mr{sketch}_R(\mr{fold}(\mr{inc}))$.  For example, the ``count''
sketch is defined as $\mr{incsketch}_{\N}(\mr{one}_{\N})$, and the
``sum'' sketch is $\mr{incsketch}_{\N}(\mr{id}_{\N})$.

\subsection{Helper data structures}

Sometimes, while we fold an increment over a set to produce a result
in $R$ we need to carry additional information, so we use a different
monoid $R'$, and some auxiliary functions to convert back and forth
from $R$ and $R'$:

$(\mr{inc}_{R'}, \mr{done}_{R'})$, where

\begin{itemize}
\item $\mr{inc}_{R'}: T \rightarrow R'$ is the increment,
\item $\mr{done}_{R'}: R' \rightarrow R$ is the function that
  extracts the final result.
\end{itemize}

The value $0_{R'}$ is used to generate the initial value for folding
over $R'$.  A new sketch operator $\mr{helpersketch}(f')$ can be
defined, as follows $\mr{helpersketch}(f'_{R'}): \tree{T^*}
\rightarrow R$, $\mr{helpersketch}(f') = \mr{sketch}_R(\mr{done}_{R'}
\circ \mr{fold}_{R'}(\mr{inc}_{R'}))$.

An example for the use of $\mr{helperketch}$ is processing a list by
sampling its elements with a given sampling rate.  In this case a
value of $R'$ can hold the result produced so far (a value from $R$),
but also the current state of the random number generator; the
\mr{inc} function skips some of the elements based on the sampling
rate, and the \mr{done} function scales the final counts by dividing
with the sampling rate.  (This scaling produces meaningful results
only when the values produced by \mr{inc} are bounded, e.g., when
counting elements.)

\subsection{Higher order incremental sketches}

Let us consider the problem of computing the ``histogram'' of a
dataset using a sketch.  A histogram divides the data into a fixed,
finite set of buckets $n$ and counts the number of elements that fall
into each bucket.  The buckets can be defined abstractly by a function
$\mr{bucket}: T \rightarrow [n]$, which assigns a value between $0$
and $n-1$ to each element of $T$.  A histogram sketch produces values
in the monoid $\N^n$ (vectors of $n$ integers, using pointwise element
addition).  We define an auxiliary (Kronecker delta) function
$\delta_{\N}$, which receives an index, and (an unused) value and
produces a vector with a single 1 value, corresponding to the index:
$\delta_{\N}: [n] \times T \rightarrow \N^n$: $\delta_{\N}(i, t) = v$,
where $
\begin{cases}
  v[j] = 0 & \forall j.i \not= j \\
  v[i] = 1
\end{cases}$.

We can then define a histogram operator parameterized with the bucket
function as the following sketch: $\mr{histogram}(\mr{bucket}) =
\mr{incsketch}(\delta_{\N} \circ \mr{bucket})$.

We can further generalize this scheme: instead of producing a vector
of numbers $\N^n$, we produce a vector of values over an arbitrary
monoid $R$.  Let us choose another function $f: T \rightarrow R$ and
define a generalized version of $\delta$ with two parameters:
$\delta_{f,R}: [n] \times T \rightarrow R^n$ function as:
$\delta_{f,R}(i, t) = v$, where $
\begin{cases}
  v[j] = 0_R & \forall j.i \not= j \\
  v[i] = f(t)
\end{cases}
$.

We can now speak of a higher-order sketch $\mr{histogram}: \tree{T}
\rightarrow R^n$, parameterized with another sketch-defining function
$f: T \rightarrow R$, defined by $\mr{histogram}(\mr{bucket}, f) =
\mr{incsketch}(\delta_{f,R} \circ bucket)$.

The standard histogram is just $\mr{incsketch}(\delta_{\mr{one}, \N}
\circ \mr{bucket})$.  Let us denote $\eta: T \rightarrow \N^n =
\delta_{\mr{one}, \N} \circ \mr{bucket}$; then the standard histogram
is $\mr{incsketch}(\eta)$.

\subsection{Applications of higher-order sketches}

\subsubsection{Group by-sum}

If we replace $\mr{one}$ with $\mr{id}$ in the above definition, we
get a different kind of histogram, that sums up all values in a
bucket, instead of counting them: $\mr{incsketch}(\delta_{\mr{id}, \N}
\circ \mr{bucket}): \tree{\N} \rightarrow \N^n$.

\subsubsection{Two-dimensional and higher-dimensional histograms}

We can even compute a two-dimensional histogram: given two bucket
functions $\mr{bucket}: T \rightarrow [n]$, and $\mr{bucket}': T
\rightarrow [m]$, $\mr{incsketch}(\delta_{\eta, \N^n} \circ
\mr{bucket}') = \mr{bucket}': T \rightarrow [m]$,
$\mr{incsketch}(\delta_{(\delta_{\mr{one}, \N} \circ \mr{bucket}),
  \N^n} \circ \mr{bucket}'): \tree{T} \rightarrow \N^{n \times m}$.
This scheme is easily generalized for higher-dimensional histograms.

\subsubsection{Random samples\label{sec:random-samples}}

Another interesting sketch uses reservoir sampling to extract random
samples from a dataset.  Reservoir sampling can be implemented as a
sketch using a randomized function $\mr{reservoir}: T^* \rightarrow
(T^n, \N)$ that extracts $n$\footnote{Or fewer, if the stream is
  short.} samples from a stream.  The second value of the tuple
returned by \mr{reservoir} is the number of elements in the stream.
Reservoir sampling initially fills up a vector of samples, and then
proceeds to use a coin biased with a probability that keeps decreasing
to decide whether to ``replace'' one of the saved samples.  Merging
two vectors $(v_0, c_0) \in (T^n, \N)$ and $(v_1, c_1) \in (T^n, \N)$
uses a biased coin with bias $c_0 / (c_0 + c_1)$ to decide whether to
keep an element from $v_0$ or $v_1$ (and sums up the counts).

The higher-order bucketing sketch can be combined with the reservoir
sketch to produce a vector of samples, one per bucket:
$\mr{incsketch}(\delta_{(\mr{reservoir}, (T^n, \N))} \circ
\mr{bucket})$.

\subsection{Other interesting sketches}

In this section we describe briefly other interesting sketches that
can be computed using this model.

\subsubsection{First-$k$}

This sketch is described by two parameters: a total order $o$ over
$T$, and a maximum size $k$.  This sketch returns the $k$\footnote{Or
  fewer if there are not enough elements.} largest elements according
to the total order: $\mr{first}(o, k): \tree{T^*} \rightarrow T^k$.
The monoid addition function is ``merge sort''.

\subsubsection{Heavy hitters}

This sketch is described by two parameters: an equality relation $o$
over $T$, and a minimum frequency $0 < f \leq 1$, and it returns all
tuples that compose at least a fraction $1/f$ of the multiset of leaf
values.  This can be implemented exactly with an algorithm such as
Mishra-Gries, or approximately by sampling.  Since Mishra-Gries can
produce false positives, a second sketch can be used to compute the
exact frequencies of the elements to eliminate the false positives.

\subsubsection{Simple statistics}

Given numeric data sketches can be used to compute simple statistics
of the data: the min, max, count, average, statistical moments
$(\sum_i e_i^k)/n$.

\subsubsection{Approximate quantiles}

Given a total order $o$ over $T$ and a parameter $k$ this sketch can
compute a set of approximate quantiles: $k$ tuples that are
approximately equi-distant in the sorted order produced by sorting all
values in the leaves.  This sketch is trivially implemented by
executing the random sampling sketch from
Section~\ref{sec:random-samples} selecting roughly $k^2$ samples,
sorting the produced results and choosing the $k$ empirical quantiles
of this distribution (the sorting and re-sampling are done as a
post-processing step on the sketch result).  This can be used to
compute the (approximate) median of a set, quartiles, deciles, etc.

\subsubsection{Approximate quantiles over distinct values}

This is the same problem as in the previous section, but in the sorted
order we only need to consider each distinct element in $T$ only once.
This can be done using min-hashing, by applying a hash function to
each value and keeping only the $k$ values with the \emph{smallest}
hash values.

\subsubsection{Hyper-log-log}

The hyper-log-log sketch can be used to compute an estimate of the distinct
elements in a multiset.

\subsubsection{Centroids}

The higher-order ``histogram'' sketch can be composed with the
``average'' to compute centroids of the values in each bucket; this is
a step of the k-means algorithm.

\subsubsection{Correlation matrix; Principal Component Analysis}

Given a set of tuples $T$ where each tuple is a vector of numbers $T =
\N^k$, the correlation $k \times k$ matrix of all the ``columns'' can
be computed using a sketch.  This matrix can then be used for
principal component analysis.

\end{document}
